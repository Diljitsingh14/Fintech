{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b6dad8f-326f-44f3-b72f-878354877c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import string\n",
    "import datetime\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "681b8167-e382-4853-a457-f2a2a4f2beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_file_gp_list = ['../Scraper/News-Money-control/final-data/gapped_data\\\\short_0.csv',\n",
    " '../Scraper/News-Money-control/final-data/gapped_data\\\\short_1.csv',\n",
    " '../Scraper/News-Money-control/final-data/gapped_data\\\\short_2.csv',\n",
    " '../Scraper/News-Money-control/final-data/gapped_data\\\\short_3.csv',\n",
    " '../Scraper/News-Money-control/final-data/gapped_data\\\\short_4.csv']\n",
    "\n",
    "temp_comb_g_n_df = []\n",
    "\n",
    "for gn_f in news_file_gp_list:\n",
    "    df = pd.read_csv(gn_f)\n",
    "    temp_comb_g_n_df.append(df)\n",
    "    \n",
    "comb_g_df = pd.concat(temp_comb_g_n_df)\n",
    "comb_g_df.drop('published_at.1',axis=1,inplace=True)\n",
    "comb_g_df.set_index('published_at',inplace=True)\n",
    "comb_g_df.dropna(subset=['description'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c325fbf-16f1-42d0-b707-c49c1bfdef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jeetc\\miniconda3\\envs\\fooocus\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeetc\\miniconda3\\envs\\fooocus\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('text-classification',model='ProsusAI/finbert')\n",
    "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('ProsusAI/Finbert') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0f559a5-7528-4ce8-ac3e-5876bf93ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define a function to process a batch of data and save sentiments\n",
    "def process_and_save_batch(batch_df, text_label, save_folder):\n",
    "    sentiments = defaultdict(lambda: {'pos': [], 'neg': [], 'neu': []})\n",
    "\n",
    "    for index, row in tqdm(batch_df.iterrows(), total=len(batch_df)):\n",
    "        text = [row[text_label][:400]]\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        outputs = model(**inputs)\n",
    "        probs = outputs.logits.softmax(dim=1)\n",
    "\n",
    "        pos = probs[:, 0].tolist()\n",
    "        neg = probs[:, 1].tolist()\n",
    "        neu = probs[:, 2].tolist()\n",
    "\n",
    "        sentiments[index]['pos'].extend(pos)\n",
    "        sentiments[index]['neg'].extend(neg)\n",
    "        sentiments[index]['neu'].extend(neu)\n",
    "\n",
    "    # Save sentiments to a file in the specified folder\n",
    "    save_path = f\"sentiments/sentiments_{save_folder}.json\".replace(\" \",\"\")\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(sentiments, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d161c7-730a-4cc4-9252-aebb9b4c9d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(range(0,1700000,100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c03aae-e75c-4f4e-ab85-8b7f160c97b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▏                                                               | 10094/100000 [7:57:45<3:44:08,  6.69it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(len(batches)):\n",
    "    print(batches[i],batches[i+1])\n",
    "    process_and_save_batch(comb_g_df[batches[i]:batches[i+1]],'description',i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
